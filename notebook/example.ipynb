{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Shallow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in snli json (can download from: https://nlp.stanford.edu/projects/snli/)\n",
    "snli = pd.read_json(\"snli_1.0_train.jsonl\", lines=True)\n",
    "#change columns and columns values to what huggingface expects\n",
    "snli = snli.rename(columns={\"sentence1\": \"premise\", \"sentence2\": \"hypothesis\", \"gold_label\":\"label\"})\n",
    "snli['label'] = snli['label'].map({'entailment': int(0), 'neutral': int(1), 'contradiction': int(2)})\n",
    "snli = snli.dropna()\n",
    "snli['label'] = snli['label'].astype('int')\n",
    "#randomly sample observations for training and evaluation sets\n",
    "sample_size = 500\n",
    "snli = snli.sample(frac=1)\n",
    "snli_bias = snli.iloc[0:sample_size, :]\n",
    "snli_bias_eval = snli.iloc[sample_size:, :]\n",
    "#save training and evaluation sets\n",
    "snli_bias.to_json(\"snli_bias_train.json\", orient='records')\n",
    "snli_bias_eval.to_json(\"snli_bias_eval.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model on small fraction of total training data\n",
    "#for the ELECTRA-small model and SNLI dataset I used 500 observations + 20 epochs\n",
    "#for a different model/dataset you will need to change number of observations or number of epochs\n",
    "#these are changed to reach desired accuracy and 'certainty' (see next few code blocks)\n",
    "#this step can take a lot of trial and error to get right\n",
    "!python run.py --do_train --task nli --dataset snli_bias_train.json --output_dir ./trained_model_bias_500_20/ --per_device_train_batch_size 256 --num_train_epochs 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on the rest of training data (this can be very slow, to save time first evaluate on much smaller subset of data)\n",
    "#goal here is to achieve 60-70% accuracy and assign most predictions with probability > 0.9\n",
    "!python run.py --do_eval --task nli --dataset snli_bias_eval.json --model ./content/trained_model_bias_500_20/ --output_dir ./eval_output_bias_500_20/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking 'certainty' (portion of predictions with probability > 0.9)\n",
    "#collecting probability of each model output\n",
    "snli_eval = pd.read_json(\"./eval_output_bias_500_20/eval_predictions.jsonl\", lines=True)\n",
    "softmax = nn.Softmax()\n",
    "l = snli_eval.shape[0]\n",
    "max_prob = np.zeros((l))\n",
    "for i in range(0, l):\n",
    "    logits = snli_eval.predicted_scores[i]\n",
    "    logits = torch.tensor(logits)\n",
    "    probs = softmax(logits)\n",
    "    probs = probs.numpy()\n",
    "    max = np.max(probs)\n",
    "    max_prob[i] = max\n",
    "max_prob = pd.Series(max_prob)\n",
    "#plot histogram of probabilities\n",
    "max_prob.hist()\n",
    "#ideally most (>50%) of predictions should have probability > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training using reweighted loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key part here is to pass eval_predictions.json in as the dataset\n",
    "#this file contains all the usual training data + predictions of shallow model\n",
    "!python run_custom.py --do_train --task nli --dataset eval_predictions.jsonl --output_dir ./trained_model_reweighted/ --per_device_train_batch_size 256 --num_train_epochs 3.0\n",
    "#to use annealing reweighted loss add '--annealing True'\n",
    "#annealing can help regain evaluation performance on biased datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating on snli eval. set\n",
    "!python run.py --do_eval --task nli --dataset snli --model ./trained_model_reweighted --output_dir ./eval_snli_reweighted\n",
    "#performace will likely be slightly worse than a model that has not been reweighted as it can no longer exploit artifacts to make easy predictions\n",
    "#However, performance should be improved on a challenge dataset (e.g. HANS https://github.com/tommccoy1/hans)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
