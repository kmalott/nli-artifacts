{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through the project and provide explanations and results along the way. It does not contain much code. To see the huggingface and pytorch code used to train and evaluate the model see './python/run.py'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The goal of this projects was to explore data artifacts found in one common dataset used to evaluate LLMs and try some techniques to mitigate their effect. Specifically, this will focus on the task of natural language inference (NLI) and the Stanford NLI (SNLI) dataset. This dataset has been shown to contian multiple artifcats but I will focus on 3 syntactic heuristics (these will be described in the next section). The two techniques used seem capable of reducing the impact of these artifacts to varying degrees, but are not without their downsides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI and SNLI\n",
    "NLI is the task of determining if a premise sentence entails (supports), is neutral to, or contradicts a hypothesis sentence. It has been used recently as one way of evaluating if LLMs are able to understand the contents of a sentence. One widely used benchmark dataset for this task is SNLI, which contains 500k+ sentence pairs. An example from SNLI is:\n",
    "\n",
    "premise: A baby at the end of a slip and slide at a party\n",
    "\n",
    "hypothesis: The baby is wet.\n",
    "\n",
    "In this example the correct output label is entailment because going down a slip and slide results in getting wet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI Artifacts\n",
    "SNLI contains many artifacts. One simple example is that hypotheses containing the word 'not' correspond to output labels of contradiction with a probability much more than random chance. This is highly problematic because any sophisticated LLM can easily pick up on basic features like these and use them to classify sentence pairs. This allows LLMs to acheive high performance without actually preforming the intended and much more challenging task of understanding the sentences to see if they support each other. \n",
    "\n",
    "The specifc artifact this project focuses on are all to do with..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANS Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reweighting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
