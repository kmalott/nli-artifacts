{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through an example use-case for the reweighted loss function. My goal is to give sufficient background information that those unfamilar with NLI can still follow along and understand the rationale for and results of changing the loss function. The work in this notebook was originally done for a final paper for a graduate class in NLP, but I wanted to repackage the content of the paper in a more accessable format, resulting in this notebook. After each section I'll add some links for the sources of each section and more relevant backgorund material for those wanting to go deeper into the topic. \n",
    "\n",
    "This notebook does not contain much code. If you would rather just see the huggingface and pytorch code used to train and evaluate the model see './python/run.py' and './python/reweighted_training.py'. For an example of how to use these files code see './notebook/example.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The goal of this projects was to explore data artifacts found in one common dataset used to evaluate LLMs and try some techniques to mitigate their effect. Specifically, this will focus on the task of natural language inference (NLI) and the Stanford NLI (SNLI) dataset. This dataset has been shown to contian multiple artifacts but I will focus on 3 syntactic heuristics (described later). I attempted to use two different techniques to reduce the impact of the dataset artifacts: adding training data, and adapting the training loss function. Adding training data preforms well but is not an ideal solution and is mostly used to show the performance shortcomings of adapting the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI and SNLI\n",
    "NLI (Natural Language Inference) is the task of determining if a premise sentence entails (supports), is neutral to, or contradicts a hypothesis sentence. It has been used recently as one way of evaluating if LLMs are able to understand the contents of a sentence. One widely used benchmark dataset for this task is SNLI, which contains 500k+ sentence pairs. An example sentence pair from SNLI is:\n",
    "\n",
    "premise: A baby at the end of a slip and slide at a party.  \\\n",
    "hypothesis: The baby is wet. \\\n",
    "label: contradiction\n",
    "\n",
    "In this example the correct output label is entailment because going down a slip and slide results in getting wet. To keep this document reasonably short I won't include entailment and neutral examples but these can be found at the SNLI dataset link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources + Additional reading: \\\n",
    "NLI paper: https://aclanthology.org/C08-1066/ \\\n",
    "SNLI paper: https://nlp.stanford.edu/pubs/snli_paper.pdf \\\n",
    "SNLI dataset: https://nlp.stanford.edu/projects/snli/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI Artifacts\n",
    "An artifact in a dataset is a feature of the dataset that unintentionally correlates with one of the output labels. LLMs trained on datasets with artifacts can be described as being biased. LLMs can use artifacts to improve their performance when making predictions because it is often far easier to detect artifacts than it is to truely understand the contents of a sentence. This is not ideal because it allows models to avoid having to deeply understand sentences to make predictions and can inflate model performance. Additionally, different datasets often don't have the same artifacts, so a model trained on one dataset with artifacts in it will not perform well when evaluated on another dataset. \n",
    "\n",
    "The SNLI dataset contains multiple types of artifacts. One simple example is that hypothesis sentences containing the word 'not' correlate to the output label of contradiction. The specific set of artifacts this project focuses on are all to do with the hypothesis and premise sentences containing a set of the same words. An example of this is the sentence pair:\n",
    "\n",
    "premise: A big brown dog swims towards the camera\n",
    "hypothesis: A dog swims towards the camera\n",
    "label: entailment\n",
    "\n",
    "The reason this is an artifact is that the majority of sentences in SNLI that contain a set of the same set of words have an output label of entailment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources + Additional reading: \\\n",
    "SNLI artifacts paper:  \\\n",
    "HANS paper:   \\\n",
    "HANS repository:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reweighting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
