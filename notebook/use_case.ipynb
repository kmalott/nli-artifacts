{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to walk through an example use-case for the reweighted loss function. My goal is to give sufficient background information that those unfamilar with NLI can still follow along and understand the rationale for and results of changing the loss function. The work in this notebook was originally done for a final paper for a graduate class in NLP, but I wanted to repackage the content of the paper in a more accessable format, resulting in this notebook. After each section I'll add some links for the sources of each section and more relevant backgorund material for those wanting to go deeper into the topic. \n",
    "\n",
    "This notebook does not contain any code. If you would rather just see the huggingface and pytorch code used to train and evaluate the model see './python/run.py' and './python/reweighted_training.py'. For an example of how to use these files see './notebook/example.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The goal of this projects was to explore data artifacts found in one common dataset used to evaluate LLMs and try some techniques to mitigate their effect. Specifically, this will focus on the task of natural language inference (NLI) and the Stanford NLI (SNLI) dataset. This dataset has been shown to contian multiple artifacts but I will focus on 3 syntactic heuristics (described later). I attempted to use two different techniques to reduce the impact of the dataset artifacts: adding training data, and adapting the training loss function. Adding training data preforms well but is not an ideal solution and is mostly used to show the performance shortcomings of adapting the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLI and SNLI\n",
    "NLI (Natural Language Inference) is the task of determining if a premise sentence entails (supports), is neutral to, or contradicts a hypothesis sentence. It has been used recently as one way of evaluating if LLMs are able to understand the contents of a sentence. One widely used benchmark dataset for this task is SNLI, which contains 500k+ sentence pairs. An example sentence pair from SNLI is:\n",
    "\n",
    "premise: A baby at the end of a slip and slide at a party.  \\\n",
    "hypothesis: The baby is wet. \\\n",
    "label: contradiction\n",
    "\n",
    "In this example the correct output label is entailment because going down a slip and slide results in getting wet. To keep this document reasonably short I won't include entailment and neutral examples but these can be found at the SNLI dataset link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources + Additional reading: \\\n",
    "NLI paper: https://aclanthology.org/C08-1066/ \\\n",
    "SNLI paper: https://nlp.stanford.edu/pubs/snli_paper.pdf \\\n",
    "SNLI dataset: https://nlp.stanford.edu/projects/snli/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNLI Artifacts\n",
    "An artifact in a dataset is a feature of the dataset that unintentionally correlates with one of the output labels. LLMs trained on datasets with artifacts can be described as being biased. LLMs can use artifacts to improve their performance when making predictions because it is often far easier to detect artifacts than it is to truely understand the contents of a sentence. This is not ideal because it allows models to avoid having to deeply understand sentences to make predictions and can inflate model performance. Additionally, different datasets often don't have the same artifacts, so a model trained on one dataset with artifacts in it will not perform well when evaluated on another dataset. \n",
    "\n",
    "The SNLI dataset contains multiple types of artifacts. One simple example is that hypothesis sentences containing the word 'not' correlate to the output label of contradiction. The specific set of artifacts this project focuses on are all to do with the hypothesis and premise sentences containing a set of the same words. The reason this is an artifact is that the majority of sentences in SNLI that contain a set of the same set of words have an output label of entailment. An example of this is the sentence pair:\n",
    "\n",
    "premise: A big brown dog swims towards the camera \\\n",
    "hypothesis: A dog swims towards the camera \\\n",
    "label: entailment\n",
    "\n",
    "Examples like the one above can further be broken down into 3 categories: lexical overlaps, constituents, and subsequences (the details of these categories are described more in the HANS paper).  \n",
    "![Table 1](./img/table1.png) \\\n",
    "Table 1 gives a count of each of these artifact categories by output label in the SNLI dataset. As the majority of examples in these categories have an output label of entailment, a model trained on the SNLI dataset will associate the presence of these categories with the entailment label. While this works well most of the time, when one of these categories is present and the output label is not entailment, the model can struggle. \n",
    "\n",
    "This might not be a huge issue on the SNLI dataset, but for other datasets it can be. One such dataset is the HANS dataset which was specifically created to highlight this issue. Unlike SNLI, the output labels are balanced for examples that fall into the 3 categories, being evenly split across the entailment and contradiction labels. Models trained on the SNLI dataset have very poor performance when evaluated on HANS often struggling to acheive much over a 50% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources + Additional reading: \\\n",
    "SNLI artifacts paper: https://aclanthology.org/C18-1198/ \\\n",
    "HANS paper: https://arxiv.org/abs/1902.01007 \\\n",
    "HANS dataset + repository: https://github.com/tommccoy1/hans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model\n",
    "The model used for the remainder of this notebook will be the ELECTRA-small model. There are many other models that will work in place of this one. I chose the ELECTRA-small model because it works reasonably well and doesn't take too long to train compared to larger models. \n",
    "\n",
    "The base model simply refers to the ELECTRA-small model trained on the SNLI dataset with no additional corrective measures. The following sections will look at ways to improve (debias) the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources + Additional reading: \\\n",
    "ELECTRA paper: https://arxiv.org/abs/2003.10555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Base Model\n",
    "#### Adding Training Data\n",
    "One straightforward way to debias a model is to simply add more training data to counter the artifact in the dataset. For instance, in our example one could add sententence pairs that contain the same set of words but have an output label of neutral or contradiction.  If enough examples are added, sentence pairs having the same set of words would no longer correlate with the output label of entailment and the artifact would be gone. This can be a very effective way to remove artifacts given a knowledge of what the artifacts in the data are, and a means of creating potentially thousands of new training examples. \n",
    "\n",
    "Fortunately, for this particular example the creators of the HANS evlaution dataset described in the last section also created a training dataset too. Training your model on SNLI + the HANS trainining dataset can result in a debiased model. In the results section this process will be refered to as annealing. \n",
    "\n",
    "#### Reweighted loss\n",
    "Another way to debias a model is to adjust the loss on the basis of how biased a particular observation is. That is to reduce the models losson biased observations so that the model does not learn from biased observations. This method has the adavantage of not needing to create more data. But how do you determine how 'biased' or not any given observation is? The method we will focus on here uses a seperate shallow model to detect which observations are biased. This model should be the exact same as the base model but it is only trained on a small subset of the training data. Only training on a small subset of data forces the model to learn simple features to classify the sentence pairs, as there simply aren't enough examples for the model to learn more meaningful deeper features. It is assumed that these simple features the shallow model learns are the biased artifacts in the dataset. \n",
    "\n",
    "The exact number of training examples and number of training epochs the shallow model is trained on are hyperparameters that need to be chosen. The original paper of these method recommends chosing them so that the resulting shallow model acheives an accuracy of 50-60% on the remainder of the training data and assigns most of its predictions with greater than 0.9 probability. In practice, it can take a lot of trial and error to achieve these ideals. Evaluating the shallow model on only a small fraction of the overall training data can save a lot of time before evaluating on the remainder of the training data to see if the ideal accuracy and probabillity assigments are met. \n",
    "\n",
    "After the shallow model has been trained and then evaluated on the remainder of the training data, we can use its predictions to alter the loss function when training the debiased model. Specifically, we can use the probability the shallow model places on the correct label to serve as a guide of how biased this observation is. When the shallow model assigns the correct label for an observation a high probability, it is likely to be a biased observation, since our shallow that only learns simple (biased) features is able to guess it correctly. This also means we should down-weight the loss for this observation since it is biased and we don't want our model to learn from it. Alternatively, when the shallow model assigns the correct label a small probability, this example is likely not to be biased and we should focus training of the reweighted model on it. Thus the standard cross-entropy loss when training the reweighted model is simply mulltiplied by (1 - (probabillity shallow model places on the correct label)) for each observation. \n",
    "\n",
    "#### Annealing\n",
    "One downside of reweighting the loss function as described above is that it will effectively reduce the size of the training dataset, becuase many observations will have less significance in training the model as their loss is down-weighted. This has a tendency to cause the reweighted models to perform worse than their biased alternatives when evaluated in-domain (meaning the evalaution set for what they were trained on). Annealing introduces a way to regain some of this performance by gradually allowing the model to learn from all observations (even the biased ones). However, by learning from biased observations, bias is introduced back into the model. The mathematical details of annealing can be found in the paper linked below and aren't crucial to understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweighted loss paper: https://arxiv.org/abs/2009.12303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "![Table 2](./img/table2.png) \\\n",
    "The above image show the overall accuracies of each model variant on evaluation set of the HANS and SNLI datasets. Unfortunately, while both reweighting methods (reweighted, annealing) regain some performace on HANS over the base model, it seems the adding training data (annealing) is far more effective. \\\n",
    "The image below shows the accuracy by artifact category and output label on the SNLI evaluation set for each model.\n",
    "![Table 3](./img/table3.png) \\\n",
    "We can see the reweighting methods are able to improve over the base model when the output label is not entailment. But once again it seems that annealing is a far more effective method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-artifacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
